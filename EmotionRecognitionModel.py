# -*- coding: utf-8 -*-
"""MMI727ProjectOwnModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SH20cRuAmZT4KUg70MVBoCmp5agG56ts
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline
from io import StringIO
import requests

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth #You may upload files to your drive and give authentication in order to read the csv.I read files from drive without any problem. 
from pydrive.drive import GoogleDrive #I hope when you run them for grading purpose you won't encounter any problem
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

import logging
logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)

data_downloaded = drive.CreateFile({'id': '1jwE7TniCTjv4qfeKly6kB2OuiZ-iZlIW'})
data_downloaded.GetContentFile('icml_face_data.csv')

data = pd.read_csv('icml_face_data.csv',header='infer')

data.head()

def prepare_dataset(dataframe, image_size={'width':48, 'height':48}):
    faces_images = []
    pixels = dataframe[' pixels'].tolist()
    for pixel_sequence in pixels:
        face = [int(pixel) for pixel in pixel_sequence.split(' ')]
        face = np.asarray(face).reshape(image_size['width'], image_size['height'])
        face = cv2.resize(face.astype('uint8'), (image_size['width'], image_size['height']))        
        faces_images.append(face.astype('float32') / 1)
    faces_images = np.asarray(faces_images)
    faces_images = np.expand_dims(faces_images, -1) # (1, 48, 48)
    emotions_labels = pd.get_dummies(dataframe['emotion']).as_matrix()
    return faces_images, emotions_labels

columns = list(data.head(0))
columns

from sklearn.model_selection import train_test_split
import cv2
emotions_labels_map = {'angry':0,
                       'disgust':1, 
                       'fear':2, 
                       'happy':3, 
                       'sad':4, 
                       'surprise':5, 
                       'neutral':6}

groups = data.groupby(' Usage')

# get group - training
training_data = groups.get_group('Training')

# get groups on training split by emotion
training_emotions = training_data.groupby('emotion')

# get group - test
test_data = groups.get_group('PublicTest')

# get groups on test split by emotion
test_emotions = test_data.groupby('emotion')

# get group - validation
validation_data = groups.get_group('PrivateTest')

# get groups on validation split by emotion
validation_emotions = validation_data.groupby('emotion')

# prepare dataset to training, test and validate model
training_faces, training_emotions = prepare_dataset(training_data)
test_faces, test_emotions = prepare_dataset(test_data)
validation_faces, validation_emotions = prepare_dataset(validation_data)

# get size dataset elements
num_samples, num_classes = training_emotions.shape

# split and randomize data
x_train_part1, x_train_part2, y_train_part1, y_train_part2 = train_test_split(training_faces, training_emotions, test_size=0.3, random_state=42)
x_train, y_train = np.vstack((x_train_part1, x_train_part2)), np.vstack((y_train_part1, y_train_part2))

x_test_part1, x_test_part2, y_test_part1, y_test_part2 = train_test_split(test_faces, test_emotions, test_size=0.3, random_state=42)
x_test, y_test = np.vstack((x_test_part1, x_test_part2)), np.vstack((y_test_part1, y_test_part2))

x_validation_part1, x_validation_part2, y_validation_part1, y_validation_part2 = train_test_split(validation_faces, validation_emotions, test_size=0.3, random_state=42)
x_validation, y_validation = np.vstack((x_validation_part1, x_validation_part2)), np.vstack((y_validation_part1, y_validation_part2))

import tensorflow as tf
print(tf.__version__)
!pip install tflearn
import tflearn
import pandas as pd
import matplotlib.image as mpimg

print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

print(y_train.shape[0], 'train labels')
print(y_test.shape[0], 'test labels')

print("Train samples:", x_train.shape, y_train.shape)
print("Test samples:", x_test.shape, y_test.shape)

y_train

classes=["angry",
                       "disgust", 
                       "fear", 
                       "happy", 
                       "sad", 
                       "surprise", 
                       "neutral"]
#subplot some figures to visiualize the input data
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i,:,:,0])
    plt.xlabel(classes[y_train[i].argmax()])
plt.show()

def central_scale_images(X_imgs, scales):
    # Various settings needed for Tensorflow operation
    boxes = np.zeros((len(scales), 4), dtype = np.float32)
    for index, scale in enumerate(scales):
        x1 = y1 = 0.5 - 0.5 * scale # To scale centrally
        x2 = y2 = 0.5 + 0.5 * scale
        boxes[index] = np.array([y1, x1, y2, x2], dtype = np.float32)
    box_ind = np.zeros((len(scales)), dtype = np.int32)
    crop_size = np.array([48, 48], dtype = np.int32)
    
    X_scale_data = []
    tf.reset_default_graph()
    X = tf.placeholder(tf.float32, shape = (1, 48, 48, 1))
    # Define Tensorflow operation for all scales but only one base image at a time
    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        
        for img_data in X_imgs:
            batch_img = np.expand_dims(img_data, axis = 0)
            scaled_imgs = sess.run(tf_img, feed_dict = {X: batch_img})
            X_scale_data.extend(scaled_imgs)
    
    X_scale_data = np.array(X_scale_data, dtype = np.int32)
    return X_scale_data

from math import ceil, floor
def get_translate_parameters(index):
    if index == 0: # Translate left 20 percent
        offset = np.array([0.0, 0.15], dtype = np.float32)
        size = np.array([48, ceil(0.85 * 48)], dtype = np.int32)
        w_start = 0
        w_end = int(ceil(0.85 * 48))
        h_start = 0
        h_end = 48
    elif index == 1: # Translate right 20 percent
        offset = np.array([0.0, -0.15], dtype = np.float32)
        size = np.array([48, ceil(0.85 * 48)], dtype = np.int32)
        w_start = int(floor((1 - 0.85) * 48))
        w_end = 48
        h_start = 0
        h_end = 48
    elif index == 2: # Translate top 20 percent
        offset = np.array([0.15, 0.0], dtype = np.float32)
        size = np.array([ceil(0.85 * 48), 48], dtype = np.int32)
        w_start = 0
        w_end = 48
        h_start = 0
        h_end = int(ceil(0.85 * 48)) 
    else: # Translate bottom 20 percent
        offset = np.array([-0.15, 0.0], dtype = np.float32)
        size = np.array([ceil(0.85 * 48), 48], dtype = np.int32)
        w_start = 0
        w_end = 48
        h_start = int(floor((1 - 0.85) * 48))
        h_end = 48 
        
    return offset, size, w_start, w_end, h_start, h_end

def translate_images_1(X_imgs):
    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)
    n_translations = 1
    X_translated_arr = []
    
    tf.reset_default_graph()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for i in range(n_translations):
            X_translated = np.zeros((len(X_imgs), 48, 48, 1), dtype = np.float32)
            X_translated.fill(1.0) # Filling background color
            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(1)
            offsets[:, :] = base_offset 
            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)
            
            glimpses = sess.run(glimpses)
            X_translated[:, h_start: h_start + size[0], w_start: w_start + size[1], :] = glimpses
            X_translated_arr.extend(X_translated)
    X_translated_arr = np.array(X_translated_arr, dtype = np.int32)
    return X_translated_arr

def translate_images_2(X_imgs):
    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)
    n_translations = 1
    X_translated_arr = []
    
    tf.reset_default_graph()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for i in range(n_translations):
            X_translated = np.zeros((len(X_imgs), 48, 48, 1), dtype = np.float32)
            X_translated.fill(1.0) # Filling background color
            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(2)
            offsets[:, :] = base_offset 
            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)
            
            glimpses = sess.run(glimpses)
            X_translated[:, h_start: h_start + size[0], w_start: w_start + size[1], :] = glimpses
            X_translated_arr.extend(X_translated)
    X_translated_arr = np.array(X_translated_arr, dtype = np.int32)
    return X_translated_arr
def translate_images_3(X_imgs):
    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)
    n_translations = 1
    X_translated_arr = []
    
    tf.reset_default_graph()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for i in range(n_translations):
            X_translated = np.zeros((len(X_imgs), 48, 48, 1), dtype = np.float32)
            X_translated.fill(1.0) # Filling background color
            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(3)
            offsets[:, :] = base_offset 
            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)
            
            glimpses = sess.run(glimpses)
            X_translated[:, h_start: h_start + size[0], w_start: w_start + size[1], :] = glimpses
            X_translated_arr.extend(X_translated)
    X_translated_arr = np.array(X_translated_arr, dtype = np.int32)
    return X_translated_arr
def translate_images_4(X_imgs):
    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)
    n_translations = 1
    X_translated_arr = []
    
    tf.reset_default_graph()
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for i in range(n_translations):
            X_translated = np.zeros((len(X_imgs), 48, 48, 1), dtype = np.float32)
            X_translated.fill(1.0) # Filling background color
            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(4)
            offsets[:, :] = base_offset 
            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)
            
            glimpses = sess.run(glimpses)
            X_translated[:, h_start: h_start + size[0], w_start: w_start + size[1], :] = glimpses
            X_translated_arr.extend(X_translated)
    X_translated_arr = np.array(X_translated_arr, dtype = np.int32)
    return X_translated_arr

def rotate_images_90(X_imgs):
    X_rotate = []
    tf.reset_default_graph()
    X = tf.placeholder(tf.float32, shape = (48, 48, 1))
    k = tf.placeholder(tf.int32)
    tf_img = tf.image.rot90(X, k = k)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for img in X_imgs:
            rotated_img = sess.run(tf_img, feed_dict = {X: img, k: 1})
            X_rotate.append(rotated_img)
        
    X_rotate = np.array(X_rotate, dtype = np.int32)
    return X_rotate
def rotate_images_270(X_imgs):
    X_rotate = []
    tf.reset_default_graph()
    X = tf.placeholder(tf.float32, shape = (48, 48, 1))
    k = tf.placeholder(tf.int32)
    tf_img = tf.image.rot90(X, k = k)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for img in X_imgs:
            rotated_img = sess.run(tf_img, feed_dict = {X: img, k: 3})
            X_rotate.append(rotated_img)
        
    X_rotate = np.array(X_rotate, dtype = np.int32)
    return X_rotate
from math import pi

def rotate_images(X_imgs, start_angle, end_angle, n_images):
    X_rotate = []
    iterate_at = 1
    
    tf.reset_default_graph()
    X = tf.placeholder(tf.float32, shape = (None, 48, 48, 1))
    radian = tf.placeholder(tf.float32, shape = (len(X_imgs)))
    tf_img = tf.contrib.image.rotate(X, radian)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
    
        for index in range(n_images):
            degrees_angle = start_angle + index * iterate_at
            radian_value = degrees_angle * pi / 180  # Convert to radian
            radian_arr = [radian_value] * len(X_imgs)
            rotated_imgs = sess.run(tf_img, feed_dict = {X: X_imgs, radian: radian_arr})
            X_rotate.extend(rotated_imgs)

    X_rotate = np.array(X_rotate, dtype = np.int32)
    return X_rotate


def flip_images(X_imgs):
    X_flip = []
    tf.reset_default_graph()
    X = tf.placeholder(tf.float32, shape = (48, 48, 1))
    tf_img1 = tf.image.flip_left_right(X)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for img in X_imgs:
            flipped_imgs = sess.run([tf_img1], feed_dict = {X: img})
            X_flip.extend(flipped_imgs)
    X_flip = np.array(X_flip, dtype = np.int32)
    return X_flip

# Produce each image at scaling of 90%, 75% and 60% of original image.
x_train_dummy=x_train

central_scaled_images1 = central_scale_images(x_train_dummy, [0.90])
x_train=np.concatenate([x_train, central_scaled_images1],axis=0 )
central_scaled_images2 = central_scale_images(x_train_dummy, [0.75])
x_train=np.concatenate([x_train, central_scaled_images2],axis=0 )
print(x_train.shape)

translated_imgs1 = translate_images_1(x_train_dummy)
x_train=np.concatenate([x_train, translated_imgs1],axis=0 )
translated_imgs2 = translate_images_2(x_train_dummy)
x_train=np.concatenate([x_train, translated_imgs2],axis=0 )
translated_imgs3 = translate_images_3(x_train_dummy)
x_train=np.concatenate([x_train, translated_imgs3],axis=0 )
translated_imgs4 = translate_images_4(x_train_dummy)
x_train=np.concatenate([x_train, translated_imgs4],axis=0 )
print(x_train.shape)

fliped_images = flip_images(x_train_dummy)
x_train=np.concatenate([x_train, fliped_images],axis=0 )
print(x_train.shape)

rotated_images_eksi30 = rotate_images(x_train_dummy, -30, -30, 1)
x_train=np.concatenate([x_train, rotated_images_eksi30],axis=0 )
print(x_train.shape)

rotated_images_arti30 =  rotate_images(x_train_dummy, 30, 30, 1)
x_train=np.concatenate([x_train, rotated_images_arti30],axis=0 )
print(x_train.shape)

y_train_dummy=y_train
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )
y_train=np.concatenate([y_train, y_train_dummy],axis=0 )

x_train_part1, x_train_part2, y_train_part1, y_train_part2 = train_test_split(x_train, y_train, test_size=0.3, random_state=42)
x_train, y_train = np.vstack((x_train_part1, x_train_part2)), np.vstack((y_train_part1, y_train_part2))

#min_max normalization, sequeezing input into 0 and 1
x_train = (x_train/255) 
x_test  = (x_test/255)

x_validation=x_validation/255

fig, ax = plt.subplots(figsize = (10, 10))
plt.subplot(2, 2, 1)
plt.imshow(x_train[11,:,:,0])
plt.title('Base Image')
plt.subplot(2, 2, 2)
plt.imshow(rotated_images_eksi30[2,:,:,-1])
plt.title('Scale = 0.90')
plt.subplot(2, 2, 3)
plt.imshow(translated_imgs2[3,:,:,0])
plt.title('Scale = 0.75')
plt.subplot(2, 2, 4)
plt.imshow(central_scaled_images2[2,:,:,0])
plt.title('Scale = 0.60')
plt.show()

y_test_cls = np.array([label.argmax() for label in y_test])
y_train_cls= np.array([label.argmax() for label in y_train])
y_validation_cls=np.array([label.argmax() for label in y_validation])
y_train_cls[0:5]

img_size=48
img_size_flat = img_size * img_size
channels=1

"""### Define placeholders
Define input and output placeholders. It is a good idea to pass dropout rate also as a placeholder. Placeholders are defined at below. Placeholder variables serve as the input to the graph that we may change each time we execute the graph. we define the placeholder variable for the input images, Next we have the placeholder variable for the true labels associated with the images that were input in the placeholder variable x. We have the placeholder variable for the true class of each image in the placeholder variable x. Lastly, we defined keep probability as a dropout probabability.
"""

# Inputs
tf.reset_default_graph()
x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, channels],name='x')
y_true =  tf.placeholder(tf.float32, shape=[None, num_classes],name='y_true')
y_true_cls = tf.placeholder(tf.int64, [None],name='y_true_cls')
dropout_prob = tf.placeholder(tf.float32,name='dropout_prob') #In this contex drop out probabability same as a keep_probabability. For example if we give 1 to this tensor we will keep the all nodes of the spesific layer. 
#(The name given is confusing)

# Dataset
dataset = tf.data.Dataset.from_tensor_slices((x, y_true))
iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)
dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')

"""
### Define network

Define the network as a function.Recommended functions are:

`
tf.nn.convolution / tf.nn.pool / tf.nn.batch_normalization / tf.nn.dropout / tf.reshape`

mathematical model applies convolution filters to the images in the placeholder variable x. Does pooling for aggregation purposes and multiplies flattened output with the weights and then adds the biases in the fully-connected layers. Of course in order to prevent over-fitting drop/out and batch normalization is applied. Batch normalization is applied in every convolution layers. Whereas, drop/out only applies in the fully-connected layer. Also weights and biases are introduced in the model."""

layer_1_conv=tf.layers.conv2d(x, filters=64, kernel_size=[3,3], strides=(1, 1),padding='same',data_format='channels_last',
                           dilation_rate=(1, 1),activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),
                           bias_initializer=tf.contrib.layers.xavier_initializer(),kernel_regularizer=None,
                           bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None)
layer_1_conv_bn=tf.layers.batch_normalization(layer_1_conv)


layer_2_conv=tf.layers.conv2d(layer_1_conv_bn, filters=64, kernel_size=[3,3], strides=(1, 1),padding='same',data_format='channels_last',
                           dilation_rate=(1, 1),activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),
                           bias_initializer=tf.contrib.layers.xavier_initializer(),kernel_regularizer=None,
                           bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None)
layer_2_conv_bn=tf.layers.batch_normalization(layer_2_conv)
layer_2_conv_pool = tf.layers.max_pooling2d(layer_2_conv_bn,pool_size=[2, 2], strides=2,  padding='same',data_format='channels_last',name=None)
layer_2_do=tf.nn.dropout(layer_2_conv_pool, dropout_prob)


layer_3_conv=tf.layers.conv2d(layer_2_do, filters=32, kernel_size=[3,3], strides=(1, 1),padding='same',data_format='channels_last',
                           dilation_rate=(1, 1),activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),
                           bias_initializer=tf.contrib.layers.xavier_initializer(),kernel_regularizer=None,
                           bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None)
layer_3_conv_bn=tf.layers.batch_normalization(layer_3_conv)


layer_4_conv=tf.layers.conv2d(layer_3_conv_bn, filters=32, kernel_size=[3,3], strides=(1, 1),padding='same',data_format='channels_last',
                           dilation_rate=(1, 1),activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),
                           bias_initializer=tf.contrib.layers.xavier_initializer(),kernel_regularizer=None,
                           bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None)
layer_4_conv_bn=tf.layers.batch_normalization(layer_4_conv)
layer_4_conv_pool = tf.layers.max_pooling2d(layer_4_conv_bn,pool_size=[2, 2], strides=2,  padding='same',data_format='channels_last',name=None)
layer_4_do=tf.nn.dropout(layer_4_conv_pool, dropout_prob)


layer_5_conv=tf.layers.conv2d(layer_4_do, filters=16, kernel_size=[3,3], strides=(1, 1),padding='same',data_format='channels_last',
                           dilation_rate=(1, 1),activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),
                           bias_initializer=tf.contrib.layers.xavier_initializer(),kernel_regularizer=None,
                           bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None)
layer_5_conv_bn=tf.layers.batch_normalization(layer_5_conv)
#layer_5_conv_pool = tf.layers.max_pooling2d(layer_5_conv_bn,pool_size=[2, 2], strides=2,  padding='same',data_format='channels_last',name=None)
#layer_5_do=tf.nn.dropout(layer_5_conv_pool, dropout_prob)

layer_6_conv=tf.layers.conv2d(layer_5_conv_bn, filters=16, kernel_size=[3,3], strides=(1, 1),padding='same',data_format='channels_last',
                           dilation_rate=(1, 1),activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),
                           bias_initializer=tf.contrib.layers.xavier_initializer(),kernel_regularizer=None,
                           bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,trainable=True,name=None)
layer_6_conv_bn=tf.layers.batch_normalization(layer_6_conv)
layer_6_conv_pool = tf.layers.max_pooling2d(layer_6_conv_bn,pool_size=[2, 2], strides=2,  padding='same',data_format='channels_last',name=None)
layer_6_do=tf.nn.dropout(layer_6_conv_pool, dropout_prob)

flat = tf.contrib.layers.flatten(layer_6_do)
full1_fc = tf.layers.dense(inputs= flat, units=512, activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())
full1_bn=tf.layers.batch_normalization(full1_fc)
full1_do = tf.nn.dropout(full1_bn, dropout_prob)

full2_fc = tf.layers.dense(inputs= full1_do, units=256, activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())
full2_bn=tf.layers.batch_normalization(full2_fc)
full2_do = tf.nn.dropout(full2_bn, dropout_prob)

full3_fc = tf.layers.dense(inputs= full2_do, units=128, activation=tf.nn.relu,use_bias=True,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())
full3_bn=tf.layers.batch_normalization(full3_fc)
full3_do = tf.nn.dropout(full3_bn, dropout_prob)

output = tf.layers.dense(inputs= full3_do, units=7, activation=None)

y_pred_cls = tf.argmax(output, axis=1)

"""### Define cost function

Define cost with respect to predictions and labels.
The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. The goal of optimization is therefore to minimize the mean of cross-entropy so it gets as close to zero as possible by changing the weights and biases of the model.
"""

cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=output,
                                                        labels=y_true)
cost = tf.reduce_mean(cross_entropy)

"""### Define optimizer"""

optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9, beta2=0.999,epsilon=1e-8).minimize(cost)

"""### Define performance measures
This is a vector of booleans whether the predicted class equals the true class of each image. It takes 1 if our prediction is correct. 0 otw.
"""

correct_prediction = tf.equal(y_pred_cls, y_true_cls)

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

"""### Create TensorFlow session"""

session = tf.Session()

"""### Initialize variables
Variables are initialized.
"""

session.run(tf.global_variables_initializer())

saver = tf.train.Saver()

"""### Define train function

"""

def train(epochs):
    i=0
    a=np.zeros(epochs)
    a1=np.zeros(epochs)
    train_pred=np.zeros(epochs)
    for epoch in range(epochs):
      batch_size = 256
      dummy=0
      acc_batch=0
      for b in range(x_train.shape[0]//batch_size): 
        x_batch = x_train[b * batch_size : (b+1) * batch_size]
        # Get a batch of training examples.
        # x_batch now holds a batch of images and
        # y_true_batch are the true labels for those images.
        y_true_batch =y_train[b * batch_size : (b+1) * batch_size]
        # Put the batch into a dict with the proper names
        # for placeholder variables in the TensorFlow graph.
        # Note that the placeholder for y_true_cls is set in order to plot training accuracy
        y_true_cls=y_train_cls[b * batch_size : (b+1) * batch_size]
        do=float(0.75)
        feed_dict_train = {x: x_batch,
                           y_true: y_true_batch,
                           dropout_prob: do
                           }

        # Run the optimizer using this batch of training data.
        # TensorFlow assigns the variables in feed_dict_train
        # to the placeholder variables and then runs the optimizer.
        session.run(optimizer, feed_dict=feed_dict_train)
        train_pred_j=session.run(y_pred_cls, feed_dict=feed_dict_train)
        correct_prediction = np.equal(train_pred_j, y_true_cls)
        acc = np.mean(correct_prediction)
        acc_batch=acc_batch+acc
        dummy=dummy+1
      train_pred[i]=acc_batch/dummy
      print("Accuracy on training-set: {0:.1%}".format(train_pred[i]))  
      a[i]=print_accuracy()
      a1[i]=i
      i=i+1
    return a,a1,train_pred

feed_dict_test = {x: x_test,
                  y_true: y_test,
                  y_true_cls: y_test_cls,
                  dropout_prob:1}

feed_dict_validation = {x: x_validation,
                  y_true: y_validation,
                  y_true_cls: y_validation_cls,
                  dropout_prob:1}

def print_accuracy():
    
    # Run the model to get predictions for test data
    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_validation)
    
    # Get true labels
    cls_true = y_validation_cls
    
    # Calculate the difference betweeb predictions and true labels
    correct_prediction = np.equal(cls_pred, cls_true)
    
    # Calculate the total accuracy
    acc = np.mean(correct_prediction)

    # Print the accuracy.
    print("Accuracy on validation-set: {0:.1%}".format(acc))
    return format(acc)

"""### Performance before training"""

print_accuracy()

"""### Performance after training"""

ep_ac=train(epochs=30)
print_accuracy()

plt.plot(ep_ac[1]+1, ep_ac[0])
plt.plot(ep_ac[1]+1,ep_ac[2])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.show()

"""### Model Evaluation"""

from sklearn.metrics import confusion_matrix

def print_confusion_matrix():
    # Get the true classifications for the test-set.
    cls_true = y_test_cls
    
    # Get the predicted classifications for the test-set.
    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)

    # Get the confusion matrix using sklearn.
    cm = confusion_matrix(y_true=cls_true,
                          y_pred=cls_pred)

    # Print the confusion matrix as text.
    print(cm)

    # Plot the confusion matrix as an image.
    plt.imshow(cm, interpolation='nearest', cmap="Blues")

def print_accuracy_test():
    
    # Run the model to get predictions for test data
    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)
    
    # Get true labels
    cls_true = y_test_cls
    
    # Calculate the difference betweeb predictions and true labels
    correct_prediction = np.equal(cls_pred, cls_true)
    
    # Calculate the total accuracy
    acc = np.mean(correct_prediction)

    # Print the accuracy.
    print("Accuracy on test-set: {0:.1%}".format(acc))
    return format(acc)

print_confusion_matrix()

print_accuracy_test()

saver.save(session,"MMI727Project.ckpt")

!ls

!pwd

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo1.png', quality=1000):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
try:
  filename = take_photo()
  print('Saved to {}'.format(filename))
  
  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

import PIL.Image

from PIL import Image, ImageFilter

def imageprepare(argv):
  """
  This function returns the pixel values.
  The imput is a png file location.
  """
  im = PIL.Image.open(argv).convert('L')
  width = float(im.size[0])
  height = float(im.size[1])
  newImage = Image.new('L', (48, 48), (0)) #creates white canvas of 48x48 pixels
  
  if width > height: #check which dimension is bigger
    height = int(round((48.0/width*height),0)) #resize height according to ratio width  
    # resize and sharpen
    img = im.resize((48,height), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)
    wtop = int(round(((48 - height)/2),0)) #caculate horizontal pozition
    newImage.paste(img, (0, wtop)) #paste resized image on white canvas
  else: 
    width = int(round((48.0/height*width),0)) #resize width according to ratio height
     # resize and sharpen
    img = im.resize((width,48), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)
    wtop = int(round(((48 - height)/2),0)) #caculate horizontal pozition
    wleft = int(round(((48 - width)/2),0)) #caculate vertical pozition
    newImage.paste(img, (wleft, 0)) #paste resized image on white canvas
  
  #newImage.save("sample.png

  tv = list(newImage.getdata()) #get pixel values
  
  #normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.
  tva = tv
  return tva
  #print(tva)

imageprepare('photo1.png')

face_test=imageprepare('photo1.png')
faces_images_test = []
faces_images_vis=[]
face_test = np.asarray(face_test).reshape(48, 48)
#face_test = cv2.resize(face_test.astype('uint8'), (48, 48))        
faces_images_test.append(face_test.astype('float32') / 255)
faces_images_vis.append(face_test.astype('float32') / 1)
faces_images_test = np.asarray(faces_images_test)
faces_images_vis = np.asarray(faces_images_test)
faces_images_test = np.expand_dims(faces_images_test, -1) # (1, 48, 48)
faces_images_vis = np.expand_dims(faces_images_vis, -1) # (1, 48, 48)

plt.imshow(faces_images_vis[0,:,:,0])

feed_dict_real = {x: faces_images_test,
                  dropout_prob:1}

cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_real)

cls_pred
classes[cls_pred[0]]

plt.imshow(faces_images_vis[0,:,:,0])
plt.xlabel(classes[cls_pred[0]])

session.run(output, feed_dict=feed_dict_real)
